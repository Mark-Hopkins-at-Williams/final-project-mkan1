{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import time\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.getenv(\"HUGGING_FACE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-3B:\n",
      "- configuration_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-3B:\n",
      "- modeling_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727d331c707d4ce886e0e3b23a0dcdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff6dbbc4e264e039dc529bd5aa2d428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"apple/OpenELM-3B\",\n",
    "            trust_remote_code=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenELMForCausalLM(\n",
       "  (transformer): OpenELMModel(\n",
       "    (token_embeddings): Embedding(32000, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=2304, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1536, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (proj_2): Linear(in_features=1536, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (1): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=2304, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1536, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=3584, bias=False)\n",
       "          (proj_2): Linear(in_features=1792, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (2): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=2304, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1536, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (proj_2): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (3): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=2304, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1536, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (proj_2): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (4): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=5632, bias=False)\n",
       "          (proj_2): Linear(in_features=2816, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (5): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=6144, bias=False)\n",
       "          (proj_2): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (6): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=6656, bias=False)\n",
       "          (proj_2): Linear(in_features=3328, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (7): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=7168, bias=False)\n",
       "          (proj_2): Linear(in_features=3584, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (8): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (proj_2): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (9): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=8704, bias=False)\n",
       "          (proj_2): Linear(in_features=4352, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (10): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (proj_2): Linear(in_features=4608, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (11): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=9728, bias=False)\n",
       "          (proj_2): Linear(in_features=4864, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (12): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=10240, bias=False)\n",
       "          (proj_2): Linear(in_features=5120, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (13): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=11264, bias=False)\n",
       "          (proj_2): Linear(in_features=5632, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (14): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=11776, bias=False)\n",
       "          (proj_2): Linear(in_features=5888, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (15): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=12288, bias=False)\n",
       "          (proj_2): Linear(in_features=6144, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (16): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=12800, bias=False)\n",
       "          (proj_2): Linear(in_features=6400, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (17): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=13312, bias=False)\n",
       "          (proj_2): Linear(in_features=6656, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (18): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=14336, bias=False)\n",
       "          (proj_2): Linear(in_features=7168, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (19): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=14848, bias=False)\n",
       "          (proj_2): Linear(in_features=7424, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (20): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=15360, bias=False)\n",
       "          (proj_2): Linear(in_features=7680, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (21): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=15872, bias=False)\n",
       "          (proj_2): Linear(in_features=7936, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (22): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (proj_2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (23): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=17408, bias=False)\n",
       "          (proj_2): Linear(in_features=8704, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (24): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=17920, bias=False)\n",
       "          (proj_2): Linear(in_features=8960, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (25): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=18432, bias=False)\n",
       "          (proj_2): Linear(in_features=9216, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (26): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=18944, bias=False)\n",
       "          (proj_2): Linear(in_features=9472, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (27): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=19456, bias=False)\n",
       "          (proj_2): Linear(in_features=9728, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (28): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=20480, bias=False)\n",
       "          (proj_2): Linear(in_features=10240, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (29): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=3840, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=2560, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=20992, bias=False)\n",
       "          (proj_2): Linear(in_features=10496, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (30): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=4608, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=21504, bias=False)\n",
       "          (proj_2): Linear(in_features=10752, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (31): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=4608, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=22016, bias=False)\n",
       "          (proj_2): Linear(in_features=11008, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (32): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=4608, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=22528, bias=False)\n",
       "          (proj_2): Linear(in_features=11264, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (33): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=4608, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=23552, bias=False)\n",
       "          (proj_2): Linear(in_features=11776, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (34): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=4608, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=24064, bias=False)\n",
       "          (proj_2): Linear(in_features=12032, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "      (35): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=4608, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=128, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=128, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (proj_2): Linear(in_features=12288, out_features=3072, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): OpenELMRMSNorm(num_features=3072, eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Llama-2-7b-hf\",\n",
    "            trust_remote_code=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_no_cot = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "A: The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "A: The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
    "A: The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
    "A: The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: The answer is 8.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cot = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
    "A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
    "A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Q: Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"{base_no_cot}\\nQ: {question}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [prompt for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompt = tokenizer(prompts)\n",
    "tokenized_prompt = torch.tensor(\n",
    "        tokenized_prompt['input_ids'],\n",
    "        device=0\n",
    "    )\n",
    "tokenized_prompt = tokenized_prompt.to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   660, 29901,  ...,  2462, 29973,    13],\n",
       "        [    1,   660, 29901,  ...,  2462, 29973,    13],\n",
       "        [    1,   660, 29901,  ...,  2462, 29973,    13],\n",
       "        ...,\n",
       "        [    1,   660, 29901,  ...,  2462, 29973,    13],\n",
       "        [    1,   660, 29901,  ...,  2462, 29973,    13],\n",
       "        [    1,   660, 29901,  ...,  2462, 29973,    13]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "# batch\n",
    "\n",
    "output_ids = model.generate(\n",
    "    tokenized_prompt,\n",
    "    max_length=1024,\n",
    "    pad_token_id=0,\n",
    ")\n",
    "generation_time = time.time() - stime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   660, 29901,  ...,  9292,  3287,   322],\n",
       "        [    1,   660, 29901,  ...,  9292,  3287,   322],\n",
       "        [    1,   660, 29901,  ...,  9292,  3287,   322],\n",
       "        ...,\n",
       "        [    1,   660, 29901,  ...,  9292,  3287,   322],\n",
       "        [    1,   660, 29901,  ...,  9292,  3287,   322],\n",
       "        [    1,   660, 29901,  ...,  9292,  3287,   322]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time:  42.75608444213867\n",
      "['A: The answer is 16.', 'A: The answer is 16.', 'A: The answer is 16.', 'A: The answer is 16.', 'A: The answer is 16.', 'A: The answer is 16.', 'A: The answer is 16.', 'A: The answer is 16.', 'A: The answer is 16.', 'A: The answer is 16.']\n"
     ]
    }
   ],
   "source": [
    "output_text = tokenizer.batch_decode(\n",
    "    output_ids,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "output_text = [output[len(prompts[i]):].split(\"\\n\")[0] for i, output in enumerate(output_text)]\n",
    "print(\"Generation time: \", generation_time)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
